{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc491549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "# !ipcluster clean\n",
    "# !ipcluster stop --all\n",
    "# !ipcluster list\n",
    "!lsof -i:24950 | awk 'NR>1{print $2}' | uniq | xargs -I{} kill -9 {}\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004dfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# !echo $CUDA_LAUNCH_BLOCKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5daeb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lincong/optimus/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ccnl-mgmt01.scc.idea: \u001b[32mINFO 04:13:44 PM cluster.py:109] {'profile': 'hgx'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-12-19 08:13:45.299640+0000', tz='UTC')\n",
       "<DirectView [0, 1, 2, 3]>\n",
       "world_size=4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/lincong/optimus/test\n",
    "%load_ext ipytorch\n",
    "# %ipt cluster -n 4 --engines MPI\n",
    "# %ipt cluster -n 4 --engines slurm\n",
    "%ipt client --profile=hgx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfc10f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f3bce80bc24f04be7ed992dd70e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] [2023-12-19 16:14:08,765] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] [2023-12-19 16:14:08,767] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] [2023-12-19 16:14:08,768] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] [2023-12-19 16:14:08,770] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,4\"\n",
    "from ipytorch import logging\n",
    "\n",
    "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from optimus import mpu\n",
    "from optimus.utils import init_parallel, release_cuda\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from ipytorch.utils import is_notebook\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import sys\n",
    "from ipytorch.utils import is_notebook\n",
    "import logging\n",
    "import deepspeed\n",
    "\n",
    "ds_logger = logging.getLogger(\"DeepSpeed\")\n",
    "ds_logger.setLevel(logging.WARNING)  # 只显示 ERROR 级别的日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2925c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not is_notebook():\n",
    "    from absl import app, flags\n",
    "    from absl.app import _run_init, parse_flags_with_usage\n",
    "\n",
    "    FLAGS = flags.FLAGS\n",
    "\n",
    "    flags.DEFINE_string(\n",
    "        \"experiment_name\", None, \"name of experiment\", short_name=\"name\"\n",
    "    )\n",
    "    flags.DEFINE_string(\"model_path\", None, \"path to model\", short_name=\"m\")\n",
    "    flags.DEFINE_string(\"dataset_path\", None, \"path to dataset\", short_name=\"d\")\n",
    "\n",
    "    args = _run_init(sys.argv, parse_flags_with_usage)\n",
    "\n",
    "else:\n",
    "\n",
    "    class FLAGS:\n",
    "        experiment_name = \"RM TRAINING NB\"\n",
    "        model_path = \"/data/hf/platypus-13b-mp2\"\n",
    "        # model_path = \"/data/hf/platypus-13b-mp2-from-pp\"\n",
    "        dataset_path = \"sustech/prm800k-parsed\"\n",
    "\n",
    "\n",
    "PATH = Path(\"/cognitive_comp/zejianxie/platy-13b-mp2\")\n",
    "# PATH = Path(\"/data/hf/platypus-13b-mp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc88dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:2] RANK 2/4 hgx022.scc.idea: \u001b[33mWARNING 04:14:17 PM parallel_unit.py:55] No model name is provided, using default name: 0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] RANK 3/4 hgx022.scc.idea: \u001b[33mWARNING 04:14:17 PM parallel_unit.py:55] No model name is provided, using default name: 0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] RANK 1/4 hgx022.scc.idea: \u001b[33mWARNING 04:14:17 PM parallel_unit.py:55] No model name is provided, using default name: 0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] RANK 0/4 hgx022.scc.idea: \u001b[33mWARNING 04:14:17 PM parallel_unit.py:55] No model name is provided, using default name: 0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] > initializing model parallel with size 2\n",
       "MPU DP: [0]\n",
       "MPU DP: [1]\n",
       "MPU DP: [2]\n",
       "MPU DP: [3]\n",
       "MPU PP: [0, 2]\n",
       "MPU PP: [1, 3]\n",
       "MPU IO: [0, 2]\n",
       "MPU MP: [0, 1]\n",
       "MPU MP: [2, 3]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topo = init_parallel(mp_size=2, pp_size=2)\n",
    "DS_PATH = Path(FLAGS.dataset_path)\n",
    "MODEL_PATH = PATH / f\"part_{mpu.get_model_parallel_rank()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb8c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:2] RANK 2/4 hgx022.scc.idea: \u001b[31mERROR 04:14:17 PM attention.py:28] Xformers >= 0.0.21 not found, when use ALIBI, back to fuse softmax\u001b[0m\n",
       "RANK 2/4 hgx022.scc.idea: \u001b[32mINFO 04:14:22 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n",
       "Some weights of LlamaForCausalLMPipePacking were not initialized from the model checkpoint at /cognitive_comp/zejianxie/platy-13b-mp2/part_0 and are newly initialized: ['llama.26.transformer_layer.mlp.w2.weight', 'llama.32.transformer_layer.mlp.w3.weight', 'llama.35.transformer_layer.input_layernorm.scale', 'llama.23.transformer_layer.mlp.w3.weight', 'llama.25.transformer_layer.attention.query_key_value.weight', 'llama.25.transformer_layer.mlp.w2.weight', 'llama.31.transformer_layer.post_attention_layernorm.scale', 'llama.35.transformer_layer.mlp.w1.weight', 'llama.26.transformer_layer.input_layernorm.scale', 'llama.39.transformer_layer.post_attention_layernorm.scale', 'llama.21.transformer_layer.attention.dense.weight', 'llama.36.transformer_layer.mlp.w3.weight', 'llama.41.final_layer_norm.scale', 'llama.26.transformer_layer.post_attention_layernorm.scale', 'llama.24.transformer_layer.attention.query_key_value.weight', 'llama.32.transformer_layer.input_layernorm.scale', 'llama.29.transformer_layer.mlp.w3.weight', 'llama.30.transformer_layer.mlp.w1.weight', 'llama.34.transformer_layer.mlp.w2.weight', 'llama.23.transformer_layer.mlp.w1.weight', 'llama.22.transformer_layer.post_attention_layernorm.scale', 'llama.25.transformer_layer.mlp.w1.weight', 'llama.27.transformer_layer.mlp.w2.weight', 'llama.32.transformer_layer.mlp.w1.weight', 'llama.22.transformer_layer.mlp.w2.weight', 'llama.34.transformer_layer.attention.query_key_value.weight', 'llama.22.transformer_layer.mlp.w3.weight', 'llama.34.transformer_layer.attention.dense.weight', 'llama.37.transformer_layer.post_attention_layernorm.scale', 'llama.23.transformer_layer.mlp.w2.weight', 'llama.31.transformer_layer.mlp.w3.weight', 'llama.29.transformer_layer.mlp.w1.weight', 'llama.34.transformer_layer.mlp.w1.weight', 'llama.33.transformer_layer.attention.query_key_value.weight', 'llama.26.transformer_layer.mlp.w1.weight', 'llama.35.transformer_layer.post_attention_layernorm.scale', 'llama.21.transformer_layer.mlp.w2.weight', 'llama.25.transformer_layer.post_attention_layernorm.scale', 'llama.28.transformer_layer.mlp.w3.weight', 'llama.31.transformer_layer.mlp.w1.weight', 'llama.38.transformer_layer.post_attention_layernorm.scale', 'llama.23.transformer_layer.post_attention_layernorm.scale', 'llama.32.transformer_layer.attention.dense.weight', 'llama.21.transformer_layer.post_attention_layernorm.scale', 'llama.28.transformer_layer.attention.dense.weight', 'llama.31.transformer_layer.mlp.w2.weight', 'llama.27.transformer_layer.attention.dense.weight', 'llama.26.transformer_layer.mlp.w3.weight', 'llama.28.transformer_layer.mlp.w2.weight', 'llama.36.transformer_layer.mlp.w2.weight', 'llama.31.transformer_layer.attention.dense.weight', 'llama.38.transformer_layer.mlp.w2.weight', 'llama.27.transformer_layer.attention.query_key_value.weight', 'llama.21.transformer_layer.mlp.w3.weight', 'llama.24.transformer_layer.mlp.w1.weight', 'llama.26.transformer_layer.attention.dense.weight', 'llama.21.transformer_layer.attention.query_key_value.weight', 'llama.24.transformer_layer.mlp.w3.weight', 'llama.32.transformer_layer.attention.query_key_value.weight', 'llama.23.transformer_layer.attention.query_key_value.weight', 'llama.25.transformer_layer.attention.dense.weight', 'llama.38.transformer_layer.attention.query_key_value.weight', 'llama.24.transformer_layer.attention.dense.weight', 'llama.29.transformer_layer.attention.query_key_value.weight', 'llama.35.transformer_layer.mlp.w2.weight', 'llama.30.transformer_layer.attention.query_key_value.weight', 'llama.37.transformer_layer.mlp.w1.weight', 'llama.24.transformer_layer.input_layernorm.scale', 'llama.40.transformer_layer.attention.dense.weight', 'llama.37.transformer_layer.input_layernorm.scale', 'llama.28.transformer_layer.input_layernorm.scale', 'llama.39.transformer_layer.mlp.w1.weight', 'llama.23.transformer_layer.attention.dense.weight', 'llama.36.transformer_layer.post_attention_layernorm.scale', 'llama.35.transformer_layer.mlp.w3.weight', 'llama.27.transformer_layer.input_layernorm.scale', 'llama.33.transformer_layer.mlp.w2.weight', 'llama.36.transformer_layer.attention.query_key_value.weight', 'llama.28.transformer_layer.attention.query_key_value.weight', 'llama.36.transformer_layer.input_layernorm.scale', 'llama.28.transformer_layer.post_attention_layernorm.scale', 'llama.37.transformer_layer.attention.query_key_value.weight', 'llama.39.transformer_layer.mlp.w3.weight', 'llama.22.transformer_layer.input_layernorm.scale', 'llama.40.transformer_layer.mlp.w1.weight', 'llama.29.transformer_layer.mlp.w2.weight', 'llama.40.transformer_layer.mlp.w2.weight', 'llama.23.transformer_layer.input_layernorm.scale', 'llama.24.transformer_layer.mlp.w2.weight', 'llama.32.transformer_layer.post_attention_layernorm.scale', 'llama.35.transformer_layer.attention.query_key_value.weight', 'llama.39.transformer_layer.mlp.w2.weight', 'llama.33.transformer_layer.post_attention_layernorm.scale', 'llama.34.transformer_layer.mlp.w3.weight', 'llama.33.transformer_layer.mlp.w3.weight', 'llama.36.transformer_layer.mlp.w1.weight', 'llama.22.transformer_layer.mlp.w1.weight', 'llama.27.transformer_layer.mlp.w3.weight', 'llama.38.transformer_layer.mlp.w1.weight', 'llama.38.transformer_layer.mlp.w3.weight', 'llama.34.transformer_layer.post_attention_layernorm.scale', 'llama.30.transformer_layer.attention.dense.weight', 'llama.27.transformer_layer.post_attention_layernorm.scale', 'llama.29.transformer_layer.post_attention_layernorm.scale', 'llama.35.transformer_layer.attention.dense.weight', 'llama.33.transformer_layer.input_layernorm.scale', 'llama.39.transformer_layer.input_layernorm.scale', 'llama.33.transformer_layer.mlp.w1.weight', 'llama.25.transformer_layer.input_layernorm.scale', 'llama.22.transformer_layer.attention.query_key_value.weight', 'llama.32.transformer_layer.mlp.w2.weight', 'llama.25.transformer_layer.mlp.w3.weight', 'llama.30.transformer_layer.input_layernorm.scale', 'llama.31.transformer_layer.input_layernorm.scale', 'llama.37.transformer_layer.mlp.w3.weight', 'llama.30.transformer_layer.mlp.w2.weight', 'llama.40.transformer_layer.mlp.w3.weight', 'llama.27.transformer_layer.mlp.w1.weight', 'llama.21.transformer_layer.input_layernorm.scale', 'llama.36.transformer_layer.attention.dense.weight', 'llama.31.transformer_layer.attention.query_key_value.weight', 'llama.40.transformer_layer.attention.query_key_value.weight', 'llama.30.transformer_layer.post_attention_layernorm.scale', 'llama.26.transformer_layer.attention.query_key_value.weight', 'llama.34.transformer_layer.input_layernorm.scale', 'llama.42.embed_out.final_linear.weight', 'llama.30.transformer_layer.mlp.w3.weight', 'llama.29.transformer_layer.attention.dense.weight', 'llama.24.transformer_layer.post_attention_layernorm.scale', 'llama.22.transformer_layer.attention.dense.weight', 'llama.38.transformer_layer.input_layernorm.scale', 'llama.29.transformer_layer.input_layernorm.scale', 'llama.37.transformer_layer.attention.dense.weight', 'llama.21.transformer_layer.mlp.w1.weight', 'llama.28.transformer_layer.mlp.w1.weight', 'llama.40.transformer_layer.input_layernorm.scale', 'llama.33.transformer_layer.attention.dense.weight', 'llama.38.transformer_layer.attention.dense.weight', 'llama.40.transformer_layer.post_attention_layernorm.scale', 'llama.39.transformer_layer.attention.query_key_value.weight', 'llama.37.transformer_layer.mlp.w2.weight', 'llama.39.transformer_layer.attention.dense.weight']\n",
       "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
       "RANK 2/4 hgx022.scc.idea: \u001b[32mINFO 04:14:48 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] RANK 3/4 hgx022.scc.idea: \u001b[31mERROR 04:14:17 PM attention.py:28] Xformers >= 0.0.21 not found, when use ALIBI, back to fuse softmax\u001b[0m\n",
       "RANK 3/4 hgx022.scc.idea: \u001b[32mINFO 04:14:22 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n",
       "Some weights of LlamaForCausalLMPipePacking were not initialized from the model checkpoint at /cognitive_comp/zejianxie/platy-13b-mp2/part_1 and are newly initialized: ['llama.34.transformer_layer.mlp.w2.weight', 'llama.22.transformer_layer.mlp.w3.weight', 'llama.40.transformer_layer.input_layernorm.scale', 'llama.28.transformer_layer.input_layernorm.scale', 'llama.34.transformer_layer.mlp.w1.weight', 'llama.21.transformer_layer.mlp.w3.weight', 'llama.40.transformer_layer.mlp.w1.weight', 'llama.22.transformer_layer.input_layernorm.scale', 'llama.32.transformer_layer.mlp.w3.weight', 'llama.28.transformer_layer.post_attention_layernorm.scale', 'llama.30.transformer_layer.attention.query_key_value.weight', 'llama.28.transformer_layer.mlp.w1.weight', 'llama.38.transformer_layer.mlp.w3.weight', 'llama.22.transformer_layer.attention.query_key_value.weight', 'llama.23.transformer_layer.post_attention_layernorm.scale', 'llama.27.transformer_layer.mlp.w3.weight', 'llama.38.transformer_layer.attention.dense.weight', 'llama.26.transformer_layer.attention.query_key_value.weight', 'llama.35.transformer_layer.input_layernorm.scale', 'llama.39.transformer_layer.mlp.w3.weight', 'llama.25.transformer_layer.mlp.w2.weight', 'llama.25.transformer_layer.mlp.w1.weight', 'llama.26.transformer_layer.mlp.w2.weight', 'llama.29.transformer_layer.mlp.w2.weight', 'llama.27.transformer_layer.post_attention_layernorm.scale', 'llama.32.transformer_layer.input_layernorm.scale', 'llama.27.transformer_layer.attention.dense.weight', 'llama.30.transformer_layer.post_attention_layernorm.scale', 'llama.40.transformer_layer.post_attention_layernorm.scale', 'llama.22.transformer_layer.post_attention_layernorm.scale', 'llama.21.transformer_layer.post_attention_layernorm.scale', 'llama.27.transformer_layer.attention.query_key_value.weight', 'llama.36.transformer_layer.mlp.w1.weight', 'llama.25.transformer_layer.attention.query_key_value.weight', 'llama.22.transformer_layer.mlp.w1.weight', 'llama.31.transformer_layer.post_attention_layernorm.scale', 'llama.24.transformer_layer.mlp.w2.weight', 'llama.28.transformer_layer.mlp.w3.weight', 'llama.35.transformer_layer.mlp.w1.weight', 'llama.31.transformer_layer.attention.dense.weight', 'llama.32.transformer_layer.mlp.w2.weight', 'llama.33.transformer_layer.post_attention_layernorm.scale', 'llama.21.transformer_layer.mlp.w2.weight', 'llama.23.transformer_layer.input_layernorm.scale', 'llama.40.transformer_layer.mlp.w3.weight', 'llama.33.transformer_layer.input_layernorm.scale', 'llama.26.transformer_layer.input_layernorm.scale', 'llama.23.transformer_layer.attention.dense.weight', 'llama.35.transformer_layer.mlp.w3.weight', 'llama.40.transformer_layer.attention.dense.weight', 'llama.39.transformer_layer.input_layernorm.scale', 'llama.33.transformer_layer.mlp.w3.weight', 'llama.36.transformer_layer.post_attention_layernorm.scale', 'llama.40.transformer_layer.mlp.w2.weight', 'llama.23.transformer_layer.mlp.w3.weight', 'llama.23.transformer_layer.mlp.w1.weight', 'llama.33.transformer_layer.attention.dense.weight', 'llama.35.transformer_layer.attention.query_key_value.weight', 'llama.39.transformer_layer.post_attention_layernorm.scale', 'llama.31.transformer_layer.mlp.w1.weight', 'llama.29.transformer_layer.mlp.w3.weight', 'llama.30.transformer_layer.input_layernorm.scale', 'llama.37.transformer_layer.post_attention_layernorm.scale', 'llama.22.transformer_layer.attention.dense.weight', 'llama.29.transformer_layer.mlp.w1.weight', 'llama.25.transformer_layer.post_attention_layernorm.scale', 'llama.24.transformer_layer.mlp.w1.weight', 'llama.33.transformer_layer.mlp.w1.weight', 'llama.33.transformer_layer.mlp.w2.weight', 'llama.31.transformer_layer.input_layernorm.scale', 'llama.21.transformer_layer.attention.query_key_value.weight', 'llama.40.transformer_layer.attention.query_key_value.weight', 'llama.32.transformer_layer.mlp.w1.weight', 'llama.30.transformer_layer.mlp.w1.weight', 'llama.31.transformer_layer.attention.query_key_value.weight', 'llama.31.transformer_layer.mlp.w3.weight', 'llama.39.transformer_layer.attention.query_key_value.weight', 'llama.37.transformer_layer.input_layernorm.scale', 'llama.37.transformer_layer.mlp.w2.weight', 'llama.21.transformer_layer.attention.dense.weight', 'llama.36.transformer_layer.attention.query_key_value.weight', 'llama.32.transformer_layer.attention.query_key_value.weight', 'llama.24.transformer_layer.attention.query_key_value.weight', 'llama.29.transformer_layer.input_layernorm.scale', 'llama.23.transformer_layer.mlp.w2.weight', 'llama.39.transformer_layer.mlp.w1.weight', 'llama.26.transformer_layer.post_attention_layernorm.scale', 'llama.21.transformer_layer.mlp.w1.weight', 'llama.25.transformer_layer.mlp.w3.weight', 'llama.36.transformer_layer.attention.dense.weight', 'llama.38.transformer_layer.post_attention_layernorm.scale', 'llama.24.transformer_layer.mlp.w3.weight', 'llama.34.transformer_layer.attention.dense.weight', 'llama.34.transformer_layer.attention.query_key_value.weight', 'llama.38.transformer_layer.input_layernorm.scale', 'llama.34.transformer_layer.mlp.w3.weight', 'llama.37.transformer_layer.mlp.w3.weight', 'llama.27.transformer_layer.input_layernorm.scale', 'llama.39.transformer_layer.attention.dense.weight', 'llama.42.embed_out.final_linear.weight', 'llama.34.transformer_layer.input_layernorm.scale', 'llama.38.transformer_layer.mlp.w1.weight', 'llama.37.transformer_layer.attention.dense.weight', 'llama.29.transformer_layer.attention.dense.weight', 'llama.35.transformer_layer.mlp.w2.weight', 'llama.26.transformer_layer.mlp.w1.weight', 'llama.38.transformer_layer.mlp.w2.weight', 'llama.37.transformer_layer.attention.query_key_value.weight', 'llama.25.transformer_layer.input_layernorm.scale', 'llama.32.transformer_layer.attention.dense.weight', 'llama.41.final_layer_norm.scale', 'llama.30.transformer_layer.mlp.w3.weight', 'llama.25.transformer_layer.attention.dense.weight', 'llama.32.transformer_layer.post_attention_layernorm.scale', 'llama.29.transformer_layer.post_attention_layernorm.scale', 'llama.24.transformer_layer.post_attention_layernorm.scale', 'llama.24.transformer_layer.input_layernorm.scale', 'llama.27.transformer_layer.mlp.w1.weight', 'llama.30.transformer_layer.mlp.w2.weight', 'llama.39.transformer_layer.mlp.w2.weight', 'llama.34.transformer_layer.post_attention_layernorm.scale', 'llama.31.transformer_layer.mlp.w2.weight', 'llama.35.transformer_layer.attention.dense.weight', 'llama.36.transformer_layer.mlp.w2.weight', 'llama.35.transformer_layer.post_attention_layernorm.scale', 'llama.30.transformer_layer.attention.dense.weight', 'llama.36.transformer_layer.input_layernorm.scale', 'llama.28.transformer_layer.attention.query_key_value.weight', 'llama.27.transformer_layer.mlp.w2.weight', 'llama.33.transformer_layer.attention.query_key_value.weight', 'llama.26.transformer_layer.mlp.w3.weight', 'llama.29.transformer_layer.attention.query_key_value.weight', 'llama.36.transformer_layer.mlp.w3.weight', 'llama.38.transformer_layer.attention.query_key_value.weight', 'llama.28.transformer_layer.attention.dense.weight', 'llama.24.transformer_layer.attention.dense.weight', 'llama.26.transformer_layer.attention.dense.weight', 'llama.22.transformer_layer.mlp.w2.weight', 'llama.23.transformer_layer.attention.query_key_value.weight', 'llama.21.transformer_layer.input_layernorm.scale', 'llama.28.transformer_layer.mlp.w2.weight', 'llama.37.transformer_layer.mlp.w1.weight']\n",
       "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
       "RANK 3/4 hgx022.scc.idea: \u001b[32mINFO 04:14:48 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] RANK 0/4 hgx022.scc.idea: \u001b[31mERROR 04:14:17 PM attention.py:28] Xformers >= 0.0.21 not found, when use ALIBI, back to fuse softmax\u001b[0m\n",
       "RANK 0/4 hgx022.scc.idea: \u001b[32mINFO 04:14:22 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n",
       "RANK 0/4 hgx022.scc.idea: \u001b[32mINFO 04:14:22 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n",
       "Some weights of LlamaForCausalLMPipePacking were not initialized from the model checkpoint at /cognitive_comp/zejianxie/platy-13b-mp2/part_0 and are newly initialized: ['llama.12.transformer_layer.attention.dense.weight', 'llama.17.transformer_layer.post_attention_layernorm.scale', 'llama.6.transformer_layer.input_layernorm.scale', 'llama.6.transformer_layer.mlp.w2.weight', 'llama.10.transformer_layer.mlp.w3.weight', 'llama.16.transformer_layer.mlp.w3.weight', 'llama.20.transformer_layer.post_attention_layernorm.scale', 'llama.5.transformer_layer.post_attention_layernorm.scale', 'llama.15.transformer_layer.post_attention_layernorm.scale', 'llama.5.transformer_layer.mlp.w1.weight', 'llama.7.transformer_layer.attention.query_key_value.weight', 'llama.9.transformer_layer.mlp.w2.weight', 'llama.4.transformer_layer.attention.dense.weight', 'llama.1.transformer_layer.mlp.w1.weight', 'llama.15.transformer_layer.input_layernorm.scale', 'llama.20.transformer_layer.input_layernorm.scale', 'llama.16.transformer_layer.input_layernorm.scale', 'llama.16.transformer_layer.post_attention_layernorm.scale', 'llama.16.transformer_layer.attention.query_key_value.weight', 'llama.17.transformer_layer.attention.query_key_value.weight', 'llama.16.transformer_layer.attention.dense.weight', 'llama.20.transformer_layer.mlp.w2.weight', 'llama.3.transformer_layer.attention.dense.weight', 'llama.14.transformer_layer.post_attention_layernorm.scale', 'llama.3.transformer_layer.mlp.w1.weight', 'llama.15.transformer_layer.mlp.w3.weight', 'llama.19.transformer_layer.post_attention_layernorm.scale', 'llama.4.transformer_layer.post_attention_layernorm.scale', 'llama.4.transformer_layer.mlp.w1.weight', 'llama.7.transformer_layer.mlp.w3.weight', 'llama.3.transformer_layer.mlp.w3.weight', 'llama.10.transformer_layer.attention.query_key_value.weight', 'llama.18.transformer_layer.mlp.w2.weight', 'llama.9.transformer_layer.attention.query_key_value.weight', 'llama.17.transformer_layer.mlp.w1.weight', 'llama.18.transformer_layer.input_layernorm.scale', 'llama.7.transformer_layer.post_attention_layernorm.scale', 'llama.5.transformer_layer.attention.dense.weight', 'llama.11.transformer_layer.mlp.w3.weight', 'llama.14.transformer_layer.attention.query_key_value.weight', 'llama.5.transformer_layer.mlp.w2.weight', 'llama.18.transformer_layer.post_attention_layernorm.scale', 'llama.12.transformer_layer.mlp.w2.weight', 'llama.3.transformer_layer.post_attention_layernorm.scale', 'llama.12.transformer_layer.post_attention_layernorm.scale', 'llama.19.transformer_layer.mlp.w2.weight', 'llama.13.transformer_layer.input_layernorm.scale', 'llama.7.transformer_layer.mlp.w2.weight', 'llama.6.transformer_layer.attention.query_key_value.weight', 'llama.2.transformer_layer.mlp.w3.weight', 'llama.4.transformer_layer.attention.query_key_value.weight', 'llama.3.transformer_layer.attention.query_key_value.weight', 'llama.13.transformer_layer.attention.dense.weight', 'llama.7.transformer_layer.attention.dense.weight', 'llama.18.transformer_layer.attention.query_key_value.weight', 'llama.11.transformer_layer.mlp.w2.weight', 'llama.2.transformer_layer.input_layernorm.scale', 'llama.14.transformer_layer.attention.dense.weight', 'llama.9.transformer_layer.input_layernorm.scale', 'llama.4.transformer_layer.mlp.w3.weight', 'llama.11.transformer_layer.attention.dense.weight', 'llama.8.transformer_layer.mlp.w1.weight', 'llama.15.transformer_layer.attention.dense.weight', 'llama.9.transformer_layer.mlp.w1.weight', 'llama.12.transformer_layer.mlp.w1.weight', 'llama.10.transformer_layer.attention.dense.weight', 'llama.3.transformer_layer.input_layernorm.scale', 'llama.14.transformer_layer.mlp.w2.weight', 'llama.9.transformer_layer.attention.dense.weight', 'llama.8.transformer_layer.post_attention_layernorm.scale', 'llama.19.transformer_layer.mlp.w3.weight', 'llama.11.transformer_layer.input_layernorm.scale', 'llama.8.transformer_layer.attention.query_key_value.weight', 'llama.4.transformer_layer.mlp.w2.weight', 'llama.15.transformer_layer.attention.query_key_value.weight', 'llama.8.transformer_layer.attention.dense.weight', 'llama.2.transformer_layer.attention.query_key_value.weight', 'llama.8.transformer_layer.mlp.w2.weight', 'llama.7.transformer_layer.input_layernorm.scale', 'llama.17.transformer_layer.mlp.w2.weight', 'llama.3.transformer_layer.mlp.w2.weight', 'llama.8.transformer_layer.mlp.w3.weight', 'llama.4.transformer_layer.input_layernorm.scale', 'llama.6.transformer_layer.mlp.w1.weight', 'llama.10.transformer_layer.post_attention_layernorm.scale', 'llama.14.transformer_layer.input_layernorm.scale', 'llama.6.transformer_layer.mlp.w3.weight', 'llama.18.transformer_layer.mlp.w3.weight', 'llama.13.transformer_layer.post_attention_layernorm.scale', 'llama.19.transformer_layer.attention.query_key_value.weight', 'llama.17.transformer_layer.input_layernorm.scale', 'llama.2.transformer_layer.mlp.w2.weight', 'llama.17.transformer_layer.attention.dense.weight', 'llama.15.transformer_layer.mlp.w1.weight', 'llama.1.transformer_layer.mlp.w2.weight', 'llama.18.transformer_layer.attention.dense.weight', 'llama.13.transformer_layer.mlp.w3.weight', 'llama.11.transformer_layer.post_attention_layernorm.scale', 'llama.2.transformer_layer.post_attention_layernorm.scale', 'llama.13.transformer_layer.mlp.w1.weight', 'llama.20.transformer_layer.attention.query_key_value.weight', 'llama.1.transformer_layer.input_layernorm.scale', 'llama.1.transformer_layer.mlp.w3.weight', 'llama.19.transformer_layer.mlp.w1.weight', 'llama.2.transformer_layer.mlp.w1.weight', 'llama.9.transformer_layer.post_attention_layernorm.scale', 'llama.17.transformer_layer.mlp.w3.weight', 'llama.20.transformer_layer.mlp.w3.weight', 'llama.5.transformer_layer.input_layernorm.scale', 'llama.14.transformer_layer.mlp.w3.weight', 'llama.6.transformer_layer.attention.dense.weight', 'llama.9.transformer_layer.mlp.w3.weight', 'llama.15.transformer_layer.mlp.w2.weight', 'llama.11.transformer_layer.mlp.w1.weight', 'llama.10.transformer_layer.mlp.w1.weight', 'llama.19.transformer_layer.attention.dense.weight', 'llama.12.transformer_layer.mlp.w3.weight', 'llama.7.transformer_layer.mlp.w1.weight', 'llama.6.transformer_layer.post_attention_layernorm.scale', 'llama.14.transformer_layer.mlp.w1.weight', 'llama.1.transformer_layer.post_attention_layernorm.scale', 'llama.8.transformer_layer.input_layernorm.scale', 'llama.5.transformer_layer.attention.query_key_value.weight', 'llama.10.transformer_layer.input_layernorm.scale', 'llama.18.transformer_layer.mlp.w1.weight', 'llama.12.transformer_layer.input_layernorm.scale', 'llama.10.transformer_layer.mlp.w2.weight', 'llama.11.transformer_layer.attention.query_key_value.weight', 'llama.13.transformer_layer.mlp.w2.weight', 'llama.20.transformer_layer.attention.dense.weight', 'llama.1.transformer_layer.attention.dense.weight', 'llama.19.transformer_layer.input_layernorm.scale', 'llama.1.transformer_layer.attention.query_key_value.weight', 'llama.2.transformer_layer.attention.dense.weight', 'llama.13.transformer_layer.attention.query_key_value.weight', 'llama.16.transformer_layer.mlp.w1.weight', 'llama.5.transformer_layer.mlp.w3.weight', 'llama.16.transformer_layer.mlp.w2.weight', 'llama.12.transformer_layer.attention.query_key_value.weight', 'llama.20.transformer_layer.mlp.w1.weight', 'llama.0.embed_in.word_embeddings.weight']\n",
       "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
       "RANK 0/4 hgx022.scc.idea: \u001b[32mINFO 04:14:48 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] RANK 1/4 hgx022.scc.idea: \u001b[31mERROR 04:14:17 PM attention.py:28] Xformers >= 0.0.21 not found, when use ALIBI, back to fuse softmax\u001b[0m\n",
       "RANK 1/4 hgx022.scc.idea: \u001b[32mINFO 04:14:22 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n",
       "RANK 1/4 hgx022.scc.idea: \u001b[32mINFO 04:14:22 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n",
       "Some weights of LlamaForCausalLMPipePacking were not initialized from the model checkpoint at /cognitive_comp/zejianxie/platy-13b-mp2/part_1 and are newly initialized: ['llama.11.transformer_layer.mlp.w1.weight', 'llama.11.transformer_layer.mlp.w3.weight', 'llama.15.transformer_layer.attention.dense.weight', 'llama.10.transformer_layer.input_layernorm.scale', 'llama.20.transformer_layer.mlp.w1.weight', 'llama.7.transformer_layer.attention.dense.weight', 'llama.4.transformer_layer.post_attention_layernorm.scale', 'llama.15.transformer_layer.mlp.w1.weight', 'llama.15.transformer_layer.input_layernorm.scale', 'llama.9.transformer_layer.attention.dense.weight', 'llama.2.transformer_layer.input_layernorm.scale', 'llama.10.transformer_layer.mlp.w3.weight', 'llama.15.transformer_layer.mlp.w3.weight', 'llama.19.transformer_layer.attention.dense.weight', 'llama.19.transformer_layer.mlp.w1.weight', 'llama.18.transformer_layer.mlp.w2.weight', 'llama.20.transformer_layer.mlp.w3.weight', 'llama.9.transformer_layer.post_attention_layernorm.scale', 'llama.19.transformer_layer.attention.query_key_value.weight', 'llama.1.transformer_layer.post_attention_layernorm.scale', 'llama.6.transformer_layer.mlp.w1.weight', 'llama.18.transformer_layer.attention.dense.weight', 'llama.16.transformer_layer.mlp.w1.weight', 'llama.3.transformer_layer.mlp.w3.weight', 'llama.14.transformer_layer.mlp.w2.weight', 'llama.2.transformer_layer.attention.query_key_value.weight', 'llama.7.transformer_layer.mlp.w3.weight', 'llama.17.transformer_layer.mlp.w3.weight', 'llama.12.transformer_layer.post_attention_layernorm.scale', 'llama.2.transformer_layer.mlp.w2.weight', 'llama.19.transformer_layer.mlp.w2.weight', 'llama.13.transformer_layer.attention.query_key_value.weight', 'llama.7.transformer_layer.post_attention_layernorm.scale', 'llama.18.transformer_layer.mlp.w1.weight', 'llama.10.transformer_layer.post_attention_layernorm.scale', 'llama.6.transformer_layer.input_layernorm.scale', 'llama.8.transformer_layer.mlp.w2.weight', 'llama.16.transformer_layer.mlp.w3.weight', 'llama.9.transformer_layer.mlp.w3.weight', 'llama.10.transformer_layer.mlp.w2.weight', 'llama.16.transformer_layer.attention.dense.weight', 'llama.1.transformer_layer.attention.dense.weight', 'llama.12.transformer_layer.mlp.w1.weight', 'llama.20.transformer_layer.attention.dense.weight', 'llama.8.transformer_layer.input_layernorm.scale', 'llama.7.transformer_layer.attention.query_key_value.weight', 'llama.12.transformer_layer.attention.query_key_value.weight', 'llama.11.transformer_layer.attention.query_key_value.weight', 'llama.9.transformer_layer.attention.query_key_value.weight', 'llama.19.transformer_layer.mlp.w3.weight', 'llama.3.transformer_layer.attention.dense.weight', 'llama.16.transformer_layer.post_attention_layernorm.scale', 'llama.0.embed_in.word_embeddings.weight', 'llama.11.transformer_layer.mlp.w2.weight', 'llama.10.transformer_layer.attention.dense.weight', 'llama.17.transformer_layer.post_attention_layernorm.scale', 'llama.1.transformer_layer.mlp.w1.weight', 'llama.3.transformer_layer.mlp.w2.weight', 'llama.1.transformer_layer.input_layernorm.scale', 'llama.3.transformer_layer.post_attention_layernorm.scale', 'llama.14.transformer_layer.attention.dense.weight', 'llama.8.transformer_layer.attention.dense.weight', 'llama.15.transformer_layer.post_attention_layernorm.scale', 'llama.12.transformer_layer.mlp.w2.weight', 'llama.12.transformer_layer.input_layernorm.scale', 'llama.18.transformer_layer.input_layernorm.scale', 'llama.1.transformer_layer.mlp.w3.weight', 'llama.8.transformer_layer.mlp.w1.weight', 'llama.1.transformer_layer.mlp.w2.weight', 'llama.20.transformer_layer.post_attention_layernorm.scale', 'llama.5.transformer_layer.post_attention_layernorm.scale', 'llama.9.transformer_layer.mlp.w2.weight', 'llama.4.transformer_layer.mlp.w1.weight', 'llama.11.transformer_layer.post_attention_layernorm.scale', 'llama.12.transformer_layer.attention.dense.weight', 'llama.6.transformer_layer.attention.query_key_value.weight', 'llama.8.transformer_layer.attention.query_key_value.weight', 'llama.14.transformer_layer.attention.query_key_value.weight', 'llama.18.transformer_layer.attention.query_key_value.weight', 'llama.15.transformer_layer.attention.query_key_value.weight', 'llama.2.transformer_layer.mlp.w3.weight', 'llama.13.transformer_layer.attention.dense.weight', 'llama.17.transformer_layer.input_layernorm.scale', 'llama.4.transformer_layer.mlp.w2.weight', 'llama.4.transformer_layer.attention.dense.weight', 'llama.9.transformer_layer.mlp.w1.weight', 'llama.2.transformer_layer.mlp.w1.weight', 'llama.1.transformer_layer.attention.query_key_value.weight', 'llama.17.transformer_layer.attention.dense.weight', 'llama.16.transformer_layer.mlp.w2.weight', 'llama.5.transformer_layer.attention.dense.weight', 'llama.6.transformer_layer.mlp.w3.weight', 'llama.16.transformer_layer.input_layernorm.scale', 'llama.20.transformer_layer.input_layernorm.scale', 'llama.13.transformer_layer.mlp.w2.weight', 'llama.7.transformer_layer.mlp.w1.weight', 'llama.2.transformer_layer.post_attention_layernorm.scale', 'llama.6.transformer_layer.attention.dense.weight', 'llama.5.transformer_layer.mlp.w1.weight', 'llama.17.transformer_layer.attention.query_key_value.weight', 'llama.6.transformer_layer.mlp.w2.weight', 'llama.18.transformer_layer.mlp.w3.weight', 'llama.17.transformer_layer.mlp.w1.weight', 'llama.10.transformer_layer.attention.query_key_value.weight', 'llama.4.transformer_layer.mlp.w3.weight', 'llama.18.transformer_layer.post_attention_layernorm.scale', 'llama.7.transformer_layer.input_layernorm.scale', 'llama.14.transformer_layer.mlp.w1.weight', 'llama.5.transformer_layer.input_layernorm.scale', 'llama.19.transformer_layer.input_layernorm.scale', 'llama.15.transformer_layer.mlp.w2.weight', 'llama.10.transformer_layer.mlp.w1.weight', 'llama.11.transformer_layer.attention.dense.weight', 'llama.7.transformer_layer.mlp.w2.weight', 'llama.14.transformer_layer.mlp.w3.weight', 'llama.3.transformer_layer.mlp.w1.weight', 'llama.5.transformer_layer.mlp.w2.weight', 'llama.2.transformer_layer.attention.dense.weight', 'llama.13.transformer_layer.mlp.w3.weight', 'llama.13.transformer_layer.post_attention_layernorm.scale', 'llama.13.transformer_layer.input_layernorm.scale', 'llama.14.transformer_layer.post_attention_layernorm.scale', 'llama.14.transformer_layer.input_layernorm.scale', 'llama.4.transformer_layer.input_layernorm.scale', 'llama.5.transformer_layer.attention.query_key_value.weight', 'llama.6.transformer_layer.post_attention_layernorm.scale', 'llama.5.transformer_layer.mlp.w3.weight', 'llama.11.transformer_layer.input_layernorm.scale', 'llama.13.transformer_layer.mlp.w1.weight', 'llama.3.transformer_layer.input_layernorm.scale', 'llama.4.transformer_layer.attention.query_key_value.weight', 'llama.12.transformer_layer.mlp.w3.weight', 'llama.9.transformer_layer.input_layernorm.scale', 'llama.19.transformer_layer.post_attention_layernorm.scale', 'llama.20.transformer_layer.mlp.w2.weight', 'llama.16.transformer_layer.attention.query_key_value.weight', 'llama.17.transformer_layer.mlp.w2.weight', 'llama.8.transformer_layer.post_attention_layernorm.scale', 'llama.8.transformer_layer.mlp.w3.weight', 'llama.20.transformer_layer.attention.query_key_value.weight', 'llama.3.transformer_layer.attention.query_key_value.weight']\n",
       "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
       "RANK 1/4 hgx022.scc.idea: \u001b[32mINFO 04:14:48 PM attention.py:213] Flash attention successfully loaded At layer0!!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
       "Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=1, data=0, model=0): 2, ProcessCoord(pipe=1, data=0, model=1): 3}\n",
       "stage=0 layers=21\n",
       "     0: LlamaInputLayerPacking\n",
       "     1: LlamaLayerPacking\n",
       "     2: LlamaLayerPacking\n",
       "     3: LlamaLayerPacking\n",
       "     4: LlamaLayerPacking\n",
       "     5: LlamaLayerPacking\n",
       "     6: LlamaLayerPacking\n",
       "     7: LlamaLayerPacking\n",
       "     8: LlamaLayerPacking\n",
       "     9: LlamaLayerPacking\n",
       "    10: LlamaLayerPacking\n",
       "    11: LlamaLayerPacking\n",
       "    12: LlamaLayerPacking\n",
       "    13: LlamaLayerPacking\n",
       "    14: LlamaLayerPacking\n",
       "    15: LlamaLayerPacking\n",
       "    16: LlamaLayerPacking\n",
       "    17: LlamaLayerPacking\n",
       "    18: LlamaLayerPacking\n",
       "    19: LlamaLayerPacking\n",
       "    20: LlamaLayerPacking\n",
       "stage=1 layers=22\n",
       "    21: LlamaLayerPacking\n",
       "    22: LlamaLayerPacking\n",
       "    23: LlamaLayerPacking\n",
       "    24: LlamaLayerPacking\n",
       "    25: LlamaLayerPacking\n",
       "    26: LlamaLayerPacking\n",
       "    27: LlamaLayerPacking\n",
       "    28: LlamaLayerPacking\n",
       "    29: LlamaLayerPacking\n",
       "    30: LlamaLayerPacking\n",
       "    31: LlamaLayerPacking\n",
       "    32: LlamaLayerPacking\n",
       "    33: LlamaLayerPacking\n",
       "    34: LlamaLayerPacking\n",
       "    35: LlamaLayerPacking\n",
       "    36: LlamaLayerPacking\n",
       "    37: LlamaLayerPacking\n",
       "    38: LlamaLayerPacking\n",
       "    39: LlamaLayerPacking\n",
       "    40: LlamaLayerPacking\n",
       "    41: LlamaLastLayerPacking\n",
       "    42: LlamaCausalLayerPacking\n",
       "  loss: <lambda>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df7051a62c24d1a8716a51272c56092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba50b12da9a7470cae0cfcbd0050bd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 2
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b030c699c90f4d58a494eb5c8232325d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 3
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cfdd3b02454f81bc4dbb8734be3bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 1
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0655c3b50f3429a96804698839447fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 0
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c763e3b2a784d678961d7a742f6b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 3
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc74b7d58e284c3dae5e35611c75f470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 1
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3093a93766974eafa1eacdf7686b7d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 0
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[output:2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa160f857a4d425e98150200ccc017f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "engine": 2
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from optimus.hf.llama.llama_packing_pipe import LlamaForCausalLMPipePacking\n",
    "from optimus.hf.llama.llama import LlamaConfig, LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "config = LlamaConfig.from_pretrained(MODEL_PATH)\n",
    "model = (\n",
    "    LlamaForCausalLMPipePacking.from_mp_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        config=config,\n",
    "        topo=topo,\n",
    "        loss_fn=lambda x: x,\n",
    "        # save_pp_dir=MODEL_PATH.parent,\n",
    "    )\n",
    "    .cuda()\n",
    "    .eval()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f3a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from optimus.utils import release_cuda\n",
    "\n",
    "release_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb67da4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] ### Instruction:\n",
       "Introduce prime number\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] ### Instruction:\n",
       "Introduce prime number\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] ### Instruction:\n",
       "Introduce prime number\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] ### Instruction:\n",
       "Introduce prime number\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "question = \"Introduce prime number\"\n",
    "header = f\"\"\"### Instruction:\\n{question}\"\"\"\n",
    "mid = \"\\n### Response:\"\n",
    "prompt = f\"{header}{mid}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbd3d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "question = \"Hello, who are you?\"\n",
    "header = f\"\"\"### Instruction:\\n{question}\"\"\"\n",
    "mid = \"\\n### Response:\"\n",
    "prompt1 = f\"{header}{mid}\"\n",
    "print(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7606a89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:8]: \u001b[0m\n",
       "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n",
       "         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n",
       "        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n",
       "        29937, 13291, 29901], device='cuda:0')"
      ]
     },
     "metadata": {
      "after": null,
      "completed": null,
      "data": {},
      "engine_id": 0,
      "engine_uuid": "f3bba588-4fadaaa75ad599a210ba5baf",
      "error": null,
      "execute_input": "i1 = tokenizer.encode(prompt, return_tensors=\"pt\")[0].cuda()\ni2 = tokenizer.encode(prompt1, return_tensors=\"pt\")[0].cuda()\ncu_len = [0, len(i1), len(i2)]\ncu_len = torch.tensor(cu_len, dtype=torch.int32, device=\"cuda\")\ncu_len = torch.cumsum(cu_len, dim=0)\ninputs = torch.cat([i1, i2], dim=0)\ninputs\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n        29937, 13291, 29901], device='cuda:0')"
       },
       "execution_count": 8,
       "metadata": {}
      },
      "follow": null,
      "msg_id": null,
      "outputs": [],
      "received": null,
      "started": null,
      "status": null,
      "stderr": "",
      "stdout": "",
      "submitted": "2023-12-19T08:15:07.624922Z"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:8]: \u001b[0m\n",
       "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n",
       "         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n",
       "        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n",
       "        29937, 13291, 29901], device='cuda:2')"
      ]
     },
     "metadata": {
      "after": null,
      "completed": null,
      "data": {},
      "engine_id": 2,
      "engine_uuid": "f872991f-b4d7dbff1456aa683d82f0e9",
      "error": null,
      "execute_input": "i1 = tokenizer.encode(prompt, return_tensors=\"pt\")[0].cuda()\ni2 = tokenizer.encode(prompt1, return_tensors=\"pt\")[0].cuda()\ncu_len = [0, len(i1), len(i2)]\ncu_len = torch.tensor(cu_len, dtype=torch.int32, device=\"cuda\")\ncu_len = torch.cumsum(cu_len, dim=0)\ninputs = torch.cat([i1, i2], dim=0)\ninputs\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n        29937, 13291, 29901], device='cuda:2')"
       },
       "execution_count": 8,
       "metadata": {}
      },
      "follow": null,
      "msg_id": null,
      "outputs": [],
      "received": null,
      "started": null,
      "status": null,
      "stderr": "",
      "stdout": "",
      "submitted": "2023-12-19T08:15:07.625354Z"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:8]: \u001b[0m\n",
       "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n",
       "         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n",
       "        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n",
       "        29937, 13291, 29901], device='cuda:1')"
      ]
     },
     "metadata": {
      "after": null,
      "completed": null,
      "data": {},
      "engine_id": 1,
      "engine_uuid": "e398f9c3-a1f21837c21ee2688346cfb7",
      "error": null,
      "execute_input": "i1 = tokenizer.encode(prompt, return_tensors=\"pt\")[0].cuda()\ni2 = tokenizer.encode(prompt1, return_tensors=\"pt\")[0].cuda()\ncu_len = [0, len(i1), len(i2)]\ncu_len = torch.tensor(cu_len, dtype=torch.int32, device=\"cuda\")\ncu_len = torch.cumsum(cu_len, dim=0)\ninputs = torch.cat([i1, i2], dim=0)\ninputs\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n        29937, 13291, 29901], device='cuda:1')"
       },
       "execution_count": 8,
       "metadata": {}
      },
      "follow": null,
      "msg_id": null,
      "outputs": [],
      "received": null,
      "started": null,
      "status": null,
      "stderr": "",
      "stdout": "",
      "submitted": "2023-12-19T08:15:07.625222Z"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:8]: \u001b[0m\n",
       "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n",
       "         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n",
       "        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n",
       "        29937, 13291, 29901], device='cuda:3')"
      ]
     },
     "metadata": {
      "after": null,
      "completed": null,
      "data": {},
      "engine_id": 3,
      "engine_uuid": "9777e6d5-49601cc0e19910d788dafab2",
      "error": null,
      "execute_input": "i1 = tokenizer.encode(prompt, return_tensors=\"pt\")[0].cuda()\ni2 = tokenizer.encode(prompt1, return_tensors=\"pt\")[0].cuda()\ncu_len = [0, len(i1), len(i2)]\ncu_len = torch.tensor(cu_len, dtype=torch.int32, device=\"cuda\")\ncu_len = torch.cumsum(cu_len, dim=0)\ninputs = torch.cat([i1, i2], dim=0)\ninputs\n",
      "execute_result": {
       "data": {
        "text/plain": "tensor([    1,   835,  2799,  4080, 29901,    13,  2928,  3518,   346,  6019,\n         1353,    13,  2277, 29937, 13291, 29901,     1,   835,  2799,  4080,\n        29901,    13, 10994, 29892,  1058,   526,   366, 29973,    13,  2277,\n        29937, 13291, 29901], device='cuda:3')"
       },
       "execution_count": 8,
       "metadata": {}
      },
      "follow": null,
      "msg_id": null,
      "outputs": [],
      "received": null,
      "started": null,
      "status": null,
      "stderr": "",
      "stdout": "",
      "submitted": "2023-12-19T08:15:07.625479Z"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "i1 = tokenizer.encode(prompt, return_tensors=\"pt\")[0].cuda()\n",
    "i2 = tokenizer.encode(prompt1, return_tensors=\"pt\")[0].cuda()\n",
    "cu_len = [0, len(i1), len(i2)]\n",
    "cu_len = torch.tensor(cu_len, dtype=torch.int32, device=\"cuda\")\n",
    "cu_len = torch.cumsum(cu_len, dim=0)\n",
    "inputs = torch.cat([i1, i2], dim=0)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e2a345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:3] [W ProcessGroupNCCL.cpp:1856] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:2] [W ProcessGroupNCCL.cpp:1856] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5986b959f4a4f629c01077adf0dc9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] [W ProcessGroupNCCL.cpp:1856] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] [W ProcessGroupNCCL.cpp:1856] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    args = (\n",
    "        inputs.cuda(),\n",
    "        cu_len.to(dtype=torch.int32, device=\"cuda\"),\n",
    "        torch.tensor(3, dtype=torch.int64, device=\"cuda\"),\n",
    "    )\n",
    "    out = model.pipe_forward(args)\n",
    "\n",
    "out_logits = out[[int(cu_len[1] - 1), int(cu_len[2] - 1)]]\n",
    "out_tokens = torch.argmax(out_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a50d482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:2] ==============================================================\n",
       "<s> ### Instruction:\n",
       "Introduce prime number\n",
       "### Response: A\n",
       "==============================================================\n",
       "<s> ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response: I\n",
       "==============================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] ==============================================================\n",
       "<s> ### Instruction:\n",
       "Introduce prime number\n",
       "### Response: A\n",
       "==============================================================\n",
       "<s> ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response: I\n",
       "==============================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] ==============================================================\n",
       "<s> ### Instruction:\n",
       "Introduce prime number\n",
       "### Response: A\n",
       "==============================================================\n",
       "<s> ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response: I\n",
       "==============================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] ==============================================================\n",
       "<s> ### Instruction:\n",
       "Introduce prime number\n",
       "### Response: A\n",
       "==============================================================\n",
       "<s> ### Instruction:\n",
       "Hello, who are you?\n",
       "### Response: I\n",
       "==============================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "i1 = torch.cat([i1, out_tokens[0].view(-1)], dim=0)\n",
    "i2 = torch.cat([i2, out_tokens[1].view(-1)], dim=0)\n",
    "print(\"==============================================================\")\n",
    "print(tokenizer.decode(i1.tolist()))\n",
    "print(\"==============================================================\")\n",
    "print(tokenizer.decode(i2.tolist()))\n",
    "print(\"==============================================================\")\n",
    "inputs = torch.cat([i1, i2], dim=0)\n",
    "\n",
    "cu_len = [0, len(i1), len(i2)]\n",
    "cu_len = torch.tensor(cu_len, dtype=torch.int32, device=\"cuda\")\n",
    "cu_len = torch.cumsum(cu_len, dim=0)\n",
    "with torch.no_grad():\n",
    "    args = (\n",
    "        inputs.cuda(),\n",
    "        cu_len.to(dtype=torch.int32, device=\"cuda\"),\n",
    "        torch.tensor(3, dtype=torch.int64, device=\"cuda\"),\n",
    "    )\n",
    "    out = model.pipe_forward(args)\n",
    "out_logits = out[[int(cu_len[1] - 1), int(cu_len[2] - 1)]]\n",
    "out_tokens = torch.argmax(out_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4902de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
